{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431ec010-bb64-4385-ae67-21dd6727a0cc",
   "metadata": {},
   "source": [
    "# Data Cleaning with Pandas and Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a6d29-57f3-449a-a864-ae75faad6f88",
   "metadata": {},
   "source": [
    "import needed packages\n",
    "\n",
    "[pathlib](https://docs.python.org/3/library/pathlib.html) - Python module to handle file system paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889da19-b8ff-4f06-a2ae-f9a4ce26eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48aee3-eb7b-4d80-bc9d-5f116e595ad1",
   "metadata": {},
   "source": [
    "Use `sys.path.append` to add parent directory to system paths so the notebook can access the `scripts` directory\n",
    "https://stackoverflow.com/a/64562179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d94d9-381f-44db-9c1a-261f3ec48082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from scripts.normalize_data import (\n",
    "    normalize_columns, \n",
    "    normalize_expedition_section_cols,\n",
    "    remove_bracket_text,\n",
    "    remove_whitespace,\n",
    "    print_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e052fab-68ce-4720-a524-d22903d7d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_nontaxa_path = Path('..', 'processed_data', 'normalized_nontaxa_list.csv')\n",
    "normalized_taxa_search_path = Path('..', 'processed_data',  'taxa_list_search.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bf7d4-1457-4ca9-b088-78c691eb38e7",
   "metadata": {},
   "source": [
    "## Working with multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4010f87-3c43-47dc-894e-9ffd50758184",
   "metadata": {},
   "source": [
    "To process multiple files, we need to get the paths for all the files. \n",
    "\n",
    "Use `Path` and `rglob` to get all the cvs in `data_clean` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd3712-9a11-4f43-90af-feabbe8c5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(Path('..', 'processed_data', 'clean_data').rglob('*.csv'))\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3be3e0-b7c1-4074-b632-91f55e2a651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1fc0d-9b81-45bd-afa3-73fe451e2a4e",
   "metadata": {},
   "source": [
    "## read files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7517124-042d-4288-a932-cf1d121741a0",
   "metadata": {},
   "source": [
    "We used `pandas.read_csv(path, dtype=str)` to read csv and treat all columns as strings. The reason why we used `dtype=str` is because `pandas.read_csv(path)`  will automatically convert the columns to strings, integers, floats, dates. This automatic conversion can change values in unexpected ways such as converting a column with integers and NaN into floats and NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d296b3-2ffb-43b9-94e4-18f8afd7abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..', 'processed_data', 'clean_data', 'Micropal_CSV_2', '362_U1480E_planktic_forams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e1fb3-24aa-4e9a-9d92-558c248721c9",
   "metadata": {},
   "source": [
    "correct integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517431c-c84e-410f-ba63-35b49d34fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, nrows=5 , dtype=str)\n",
    "df['Pulleniatina coiling (dextral)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7eb3d-2480-4ea9-a12d-69eab7a69e7f",
   "metadata": {},
   "source": [
    "pandas automatically converts the integers to floats because of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6660262-a922-4eea-a2ff-584bf9892a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, nrows=5)\n",
    "df['Pulleniatina coiling (dextral)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3609a84-e6ff-4612-93f8-2bdf0c107cab",
   "metadata": {},
   "source": [
    "## viewing changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a56c9a-7768-4b45-91be-5c9dd83e0601",
   "metadata": {},
   "source": [
    "One thing that we found helpful when data cleaning is to see the dataframe and the total number of rows and columns.\n",
    "\n",
    "`print_df` is a custom function that calls `pd.DataFrame.shape` and `pd.DataFrame.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17d3aa-ca61-4306-bf05-572e6b142b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..', 'processed_data', 'clean_data', 'Micropal_CSV_2', '362_U1480E_planktic_forams.csv')\n",
    "df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bacbb6-7ca1-46e6-b879-dd5f3289e2ec",
   "metadata": {},
   "source": [
    "## Basic cleanup pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c0142-bd7e-449e-96f9-51210f67eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    \n",
    "    # code to change file   \n",
    "    \n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c406300-17bc-4dcd-a8a2-a19c9102dbfc",
   "metadata": {},
   "source": [
    "## Basic file cleanup\n",
    "\n",
    "pandas has methods that can be used to do some basic file cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c489129-d83d-4369-a618-0065e0ce9840",
   "metadata": {},
   "source": [
    "- delete dataframe column if all values are NA \n",
    "\n",
    "  dropna(axis='columns', how='all', inplace=True) - [pandas.DataFrame.dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "\n",
    "\n",
    "\n",
    "- delete dataframe row if all values are NA \n",
    "\n",
    "  dropna(axis='index', how='all', inplace=True) - [pandas.DataFrame.dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "\n",
    "- remove duplicate rows in dataframe \n",
    "\n",
    "  drop_duplicates(inplace=True) - [pandas.DataFrame.drop_duplicates](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ba93f-23c0-43d6-b2e7-3ffba0661704",
   "metadata": {},
   "source": [
    "before cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1b5f1-bd7d-41ae-9b41-422994334f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..', 'processed_data', 'clean_data', 'Micropal_CSV_3', '341_planktic_forams_U1417B.csv')\n",
    "\n",
    "df = pd.read_csv(path, dtype=str)\n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339148e6-dd28-45ee-aa01-cca976118158",
   "metadata": {},
   "source": [
    "after cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1546f47-b415-4a62-8ad2-e10d741acae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns', how='all', inplace=True)  \n",
    "df.dropna(axis='index', how='all', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b940d45-f485-4a6c-90cd-4cd4ef0d6103",
   "metadata": {},
   "source": [
    "Use `for` loop to run basic cleanup on all files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d7d5a-5e0e-4e4a-91ee-14567087f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    \n",
    "    df.dropna(axis='columns', how='all', inplace=True)  \n",
    "    df.dropna(axis='index', how='all', inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad25d0a-71c0-4ad9-a2f5-8c732a3602c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## remove leading and trailing white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d934a1-166a-4962-937e-68ecb5143495",
   "metadata": {},
   "source": [
    "We created a custom function `remove_whitespace` to remove all leading and trailing white spaces from a dataframe. \n",
    "\n",
    "Since we wanted to remove white spaces from the headers, we used `read_csv(header=None)` and `to_csv(header=False)` so that pandas treat the first row like any other row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93324e95-c138-4430-80d1-8b391ab0e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(paths[0], dtype=str, header=None)\n",
    "\n",
    "remove_whitespace(df)\n",
    "\n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a028c-d272-432b-9e2e-102311819299",
   "metadata": {},
   "source": [
    "remove white space from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a6fc7-3b62-48d0-b59a-24336497151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str, header=None)\n",
    "    \n",
    "    remove_whitespace(df)\n",
    "    \n",
    "    df.to_csv(path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae74e5-aa85-42f0-b12d-7c74a5d43926",
   "metadata": {},
   "source": [
    "## Normalizing columns names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268b5a9-fbe7-4986-980b-06dcf7e95f89",
   "metadata": {},
   "source": [
    "For the expedition 312 and later, the researchers for each expedition  determined the format of their data files. This resulted in a lot of variability in the file columns.  Another challenge with parsing the files is that each taxa is stored as separate column in the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe563b-d86f-4a68-baeb-ccd695f11de5",
   "metadata": {},
   "source": [
    "### get all unique column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af916c7e-1240-4298-8abf-d2d9021d1584",
   "metadata": {},
   "source": [
    "In order to normalize the header header names, we needed to get all the headers for all the files. \n",
    "\n",
    "Since we only need the header names, use `nrow=0` with `read_csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a964f-a500-4422-af94-f3ddb6458a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(paths[1], dtype=str, nrows=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922f338-af67-4f2a-8600-bf15488903a0",
   "metadata": {},
   "source": [
    "I used `pandas.DataFrame.columns()` and python `set` to get all the unique columns fo all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dddc5d-8f34-4f98-8793-d46664159fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = set()\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str, nrows=0)\n",
    "    \n",
    "    all_columns.update(df.columns)\n",
    "    \n",
    "len(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7419cd3-8eee-4835-aefd-d581164f0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7a607-e2c1-47b7-9674-031bd627a2d9",
   "metadata": {},
   "source": [
    "I then manually separate taxa names from other headers so that we could do some more processing on the taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0873975-b54e-42fb-a979-05fef12e24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_columns = {\n",
    "    'Candeina nitida',\n",
    "    'Dentoglobigerina altispira _T_ _PL5',\n",
    "    'Dentoglobigerina altispira _T_ _PL5_',\n",
    "    'Dextral:Sinistral _P. obliquiloculata_',\n",
    "    'Dextral:Sinistral _P. praecursor_',\n",
    "    'Dextral:Sinistral _P. primalis_',\n",
    "    'Globigerina bulloides',\n",
    "    'Globigerina cf. woodi',\n",
    "    'Globigerina falconensis',\n",
    "    'Globigerina umbilicata',\n",
    "    'Globigerinella aequilateralis',\n",
    "    'Globigerinella calida',\n",
    "    'Globigerinella calida _B',\n",
    "    'Globigerinella calida _B_',\n",
    "    'Globigerinella obesa',\n",
    "    'Globigerinita glutinata',\n",
    "    'Globigerinita parkerae',\n",
    "    'Globigerinita uvula',\n",
    "    'Globigerinoides bulloideus',\n",
    "    'Globigerinoides conglobatus',\n",
    "    'Globigerinoides extremus _T and B',\n",
    "    'Globigerinoides extremus _T and B_',\n",
    "    'Globigerinoides fistulosus',\n",
    "    'Globigerinoides obliquus _T',\n",
    "    'Globigerinoides obliquus _T_',\n",
    "    'Globigerinoides quadrilobatus',\n",
    "    'Globigerinoides ruber',\n",
    "    'Globigerinoides ruber (pink)',\n",
    "    'Globigerinoides ruber (white)',\n",
    "    'Globigerinoides ruber _pink_ T',\n",
    "    'Globigerinoides ruber _pink_ _T_',\n",
    "    'Globigerinoides sacculifer',\n",
    "    'Globigerinoides sacculifer (without sack)',\n",
    "    'Globigerinoides tenellus',\n",
    "    'Globigerinoides trilobus',\n",
    "    'Globigerinoidesella fistulosa _T and B_ _Pt1a',\n",
    "    'Globigerinoidesella fistulosa _T and B_ _Pt1a_',\n",
    "    'Globoconella miozea',\n",
    "    'Globorotalia (Globoconella) inflata',\n",
    "    'Globorotalia (Globorotalia) tumida tumida',\n",
    "    'Globorotalia (Hirsutella) hirsuta',\n",
    "    'Globorotalia (Hirsutella) scitula',\n",
    "    'Globorotalia (Truncorotalia) crossaformis',\n",
    "    'Globorotalia (Truncorotalia) truncatulinoides',\n",
    "    'Globorotalia anfracta',\n",
    "    'Globorotalia crassaformis',\n",
    "    'Globorotalia crassaformis sensu lato',\n",
    "    'Globorotalia flexuosa',\n",
    "    'Globorotalia flexuosa _T and B_',\n",
    "    'Globorotalia hessi',\n",
    "    'Globorotalia hessi _B_',\n",
    "    'Globorotalia hirsuta',\n",
    "    'Globorotalia inflata',\n",
    "    'Globorotalia limbata _B',\n",
    "    'Globorotalia limbata _B_',\n",
    "    'Globorotalia limbata _T_',\n",
    "    'Globorotalia margaritae _T and B_ _PL3',\n",
    "    'Globorotalia margaritae _T and B_ _PL3_',\n",
    "    'Globorotalia menardii',\n",
    "    'Globorotalia multicamerata _T',\n",
    "    'Globorotalia multicamerata _T_',\n",
    "    'Globorotalia plesiotumida _B_ _M13b_',\n",
    "    'Globorotalia plesiotumida _T',\n",
    "    'Globorotalia plesiotumida _T_',\n",
    "    'Globorotalia pseudomiocenica _T_ _PL6',\n",
    "    'Globorotalia pseudomiocenica _T_ _PL6_',\n",
    "    'Globorotalia scitula',\n",
    "    'Globorotalia tosaensis',\n",
    "    'Globorotalia tosaensis _T and B_ _Pt1b',\n",
    "    'Globorotalia tosaensis _T and B_ _Pt1b_',\n",
    "    'Globorotalia truncatulinoides',\n",
    "    'Globorotalia truncatulinoides _B',\n",
    "    'Globorotalia truncatulinoides _B_',\n",
    "    'Globorotalia tumida',\n",
    "    'Globorotalia tumida _B_ _PL1a_',\n",
    "    'Globoturborotalita apertura _T and B',\n",
    "    'Globoturborotalita apertura _T and B_',\n",
    "    'Globoturborotalita decoraperta _T and B',\n",
    "    'Globoturborotalita decoraperta _T and B_',\n",
    "    'Globoturborotalita rubescens',\n",
    "    'Neogloboquadrina acostaensis',\n",
    "    'Neogloboquadrina acostaensis (dextral)',\n",
    "    'Neogloboquadrina cf. pachyderma',\n",
    "    'Neogloboquadrina dutertrei',\n",
    "    'Neogloboquadrina humerosa',\n",
    "    'Neogloboquadrina incompta (dextral)',\n",
    "    'Neogloboquadrina inglei',\n",
    "    'Neogloboquadrina kagaensis',\n",
    "    'Neogloboquadrina nympha',\n",
    "    'Neogloboquadrina pachyderma (dextral)',\n",
    "    'Neogloboquadrina pachyderma (sin)',\n",
    "    'Neogloboquadrina pachyderma (sinistral)',\n",
    "    'Neogloboquadrina pachyderma B (sinistral, inflated form)',\n",
    "    'Neogloboquadrina pachyderma(dex)',\n",
    "    'Orbulina universa',\n",
    "    'Pulleniatina coiling (dextral)',\n",
    "    'Pulleniatina coiling (sinistral)',\n",
    "    'Pulleniatina finalis',\n",
    "    'Pulleniatina finalis _B',\n",
    "    'Pulleniatina finalis _B_',\n",
    "    'Pulleniatina obliquiloculata',\n",
    "    'Pulleniatina obliquiloculata (D)',\n",
    "    'Pulleniatina praecursor',\n",
    "    'Pulleniatina praespectabilis',\n",
    "    'Pulleniatina primalis  _Tand B',\n",
    "    'Pulleniatina primalis  _Tand B_',\n",
    "    'Sphaeroidinella dahiscens sensu lato',\n",
    "    'Sphaeroidinella dehiscens',\n",
    "    'Sphaeroidinella dehiscens s.l.',\n",
    "    'Sphaeroidinella dehiscens sensu lato _B_',\n",
    "    'Sphaeroidinellopsis kochi _T',\n",
    "    'Sphaeroidinellopsis kochi _T_',\n",
    "    'Sphaeroidinellopsis seminulina _T_ _PL4',\n",
    "    'Sphaeroidinellopsis seminulina _T_ _PL4_',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b94b27-01e3-4736-950e-f49d4d5c354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(taxa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6186-4d34-410f-9d29-5f3191bcdca1",
   "metadata": {},
   "source": [
    "Since both `all_columns` and `taxa_columns` are sets, we can subtract them to get the nontaxa headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978f7de-7d9c-49ba-8a19-f8331a311f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontaxa_columns = all_columns - taxa_columns\n",
    "\n",
    "nontaxa_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb4c23-bdd6-4975-83ce-69e965be77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nontaxa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff269b8-9656-45d5-8619-0108a24733b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create taxa and non-taxa file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bff98-4362-4b90-a99c-81473ee383ca",
   "metadata": {},
   "source": [
    "I saved the the taxa and nontaxa headers to csv so that I can access them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ee29c-65d1-4e3d-8a52-4d0ebc992dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_df = pd.DataFrame(taxa_columns, columns=['verbatim_name'])\n",
    "taxa_df.sort_values('verbatim_name', inplace=True)\n",
    "\n",
    "print_df(taxa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34c9ba-911b-4038-abbe-bc400cece164",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..', 'processed_data', 'drafts', 'taxa_list.csv')\n",
    "taxa_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ba39d-f50d-4560-a334-7868f9d68d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_taxa_df = pd.DataFrame(nontaxa_columns, columns=['field'])\n",
    "non_taxa_df.sort_values('field', inplace=True)\n",
    "\n",
    "print_df(non_taxa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f63d4-127e-434a-9bf3-592d41cb14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..', 'processed_data', 'drafts', 'nontaxa_list.csv')\n",
    "non_taxa_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36ae29-c7a3-4b09-b31f-f2b5ede98e1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### normalize headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714444e3-8b4f-49d9-b9c3-3285e31acbad",
   "metadata": {},
   "source": [
    "After the project PIs manually normalized the columns, we need to update the data files with the noramlized columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265f1ac-9298-453d-9bd4-6715a02b1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontaxa_df = pd.read_csv(normalized_nontaxa_path, dtype=str)\n",
    "print_df(nontaxa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71655b85-d6f6-46f0-b3e0-0181ca46173f",
   "metadata": {},
   "source": [
    "create a dictionary that lists the original field name and normalized field name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e90c5b-82ab-486c-b14d-04a620c0ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontaxa_mapping = nontaxa_df.set_index('field').to_dict()['normalized_field']\n",
    "nontaxa_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf00b88-4325-437b-b137-f981c777d938",
   "metadata": {},
   "source": [
    "`normalize_columns` updates the column names for a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f5c0f-a3bc-47a7-bf31-1c6f3659f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(paths[0], dtype=str)    \n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6294b1-b5f5-43d9-a340-72f68261caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(paths[0], dtype=str) \n",
    "normalize_columns(df, nontaxa_mapping)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf368eb9-56d1-44ac-a7e8-dc4eba6edd1a",
   "metadata": {},
   "source": [
    "normalize columns for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e78a3-70ea-46c5-8919-a082dd13a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)    \n",
    "    \n",
    "    normalize_columns(df, nontaxa_mapping)\n",
    "    \n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301d154-bcf7-498e-a53d-22d4f3c154d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean up row values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b48f6-59c7-46b9-be99-1f8d013a666c",
   "metadata": {},
   "source": [
    "`remove_bracket_text` removes the [text] values at the end of some taxa columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e563a0-4947-43c4-a339-96496221efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(paths[0], dtype=str)    \n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016ba37-542f-4c1d-8e4f-96d04a02d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(paths[0], dtype=str) \n",
    "df = remove_bracket_text(df)\n",
    "print_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee2cbc-1606-46c4-9aab-df0fed3d3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    \n",
    "    df = remove_bracket_text(df)\n",
    "    \n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d5fb7-0647-4e69-8c47-eeed6c901306",
   "metadata": {},
   "source": [
    "## Turn one column into multiple columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13bffb-fc8e-475c-a8c2-abceb1f2d313",
   "metadata": {},
   "source": [
    "For some files, `Sample` column was given, but `Exp, Site, Hole, Core, Type, Section, A/W` columns where not given. \n",
    "\n",
    "Sample: 363-U1483A-1H-2-W 75/77-FORAM  \n",
    "Exp: 363, Site: U1483, Hole: A, Core: 1, Type: H, Section: 2, A/W: W\n",
    "\n",
    "create `normalize_expedition_section_cols` tp convert `Sample` into separate `Exp, Site, Hole, Core, Type, Section, A/W` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c772dac-baaa-4341-9659-507cb0381522",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)   \n",
    "    \n",
    "    df = normalize_expedition_section_cols(df)\n",
    "    \n",
    "    df.to_csv(path, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab453e2b-462f-4c8f-ad5f-5a50f9e89246",
   "metadata": {},
   "source": [
    "## check if mandatory columns exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cbced-57f8-4481-902d-bfc14e22cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = {\n",
    " 'A/W',\n",
    " 'Bottom [cm]',\n",
    " 'Bottom Depth [m]',\n",
    " 'Core',\n",
    " 'Exp',\n",
    " 'Hole',\n",
    " 'Sample',\n",
    " 'Section',\n",
    " 'Site',\n",
    " 'Top [cm]',\n",
    " 'Top Depth [m]',\n",
    " 'Type'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471b3f1-2b27-4e68-a58b-27c27b1e42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path, dtype=str)    \n",
    "    cols = set(df.columns)\n",
    "    diff = required_columns - cols\n",
    "    \n",
    "    if(len(diff) > 0):\n",
    "        print(path)\n",
    "        print(required_columns - cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae0a31-be50-4a16-8a73-41ddab608cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
